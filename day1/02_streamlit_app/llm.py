# llm.py
import os
import torch
from transformers import pipeline
import streamlit as st
import time
from config import MODEL_NAME as DEFAULT_MODEL
from huggingface_hub import login

# ãƒ¢ãƒ‡ãƒ«ã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã—ã¦å†åˆ©ç”¨
@st.cache_resource
def load_model(selected_model: str):
    """LLMãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰ã™ã‚‹"""
    try:
        # Hugging Face ã®ãƒˆãƒ¼ã‚¯ãƒ³ã§ãƒ­ã‚°ã‚¤ãƒ³
        hf_token = st.secrets["huggingface"]["token"]
        login(token=hf_token)

        # ä½¿ç”¨ãƒ‡ãƒã‚¤ã‚¹ã®åˆ¤å®š
        device = 0 if torch.cuda.is_available() else -1
        st.info(f"Using device: {'cuda' if device == 0 else 'cpu'}")

        # ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã®ç”Ÿæˆ
        pipe = pipeline(
            "text-generation",
            model=selected_model,
            device=device,
            torch_dtype=torch.bfloat16 if device == 0 else None,
            # trust_remote_code=True  # å¿…è¦ã«å¿œã˜ã¦æœ‰åŠ¹åŒ–
        )

        st.success(f"ãƒ¢ãƒ‡ãƒ« '{selected_model}' ã®èª­ã¿è¾¼ã¿ã«æˆåŠŸã—ã¾ã—ãŸã€‚")
        return pipe

    except Exception as e:
        st.error(f"ãƒ¢ãƒ‡ãƒ« '{selected_model}' ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
        return None

def select_model_ui():
    """ã‚µã‚¤ãƒ‰ãƒãƒ¼ã«ãƒ¢ãƒ‡ãƒ«é¸æŠUIã‚’è¿½åŠ ã—ã¦ã€é¸æŠã—ãŸãƒ¢ãƒ‡ãƒ«åã‚’è¿”ã™"""
    st.sidebar.header("ğŸ”„ ãƒ¢ãƒ‡ãƒ«é¸æŠ")
    # ã“ã“ã«å¥½ããªãƒ¢ãƒ‡ãƒ«åã‚’è¿½åŠ ã—ã¦ãã ã•ã„
    model_options = [
        DEFAULT_MODEL,
        "gpt2",
        "gpt2-medium",
        "distilgpt2",
        "meta-llama/Llama-2-7b-chat-hf",
    ]
    return st.sidebar.selectbox("ä½¿ç”¨ã™ã‚‹LLMãƒ¢ãƒ‡ãƒ«", model_options, index=model_options.index(DEFAULT_MODEL))

def generate_response(pipe, user_question):
    """LLMã‚’ä½¿ç”¨ã—ã¦è³ªå•ã«å¯¾ã™ã‚‹å›ç­”ã‚’ç”Ÿæˆã™ã‚‹"""
    if pipe is None:
        return "ãƒ¢ãƒ‡ãƒ«ãŒãƒ­ãƒ¼ãƒ‰ã•ã‚Œã¦ã„ãªã„ãŸã‚ã€å›ç­”ã‚’ç”Ÿæˆã§ãã¾ã›ã‚“ã€‚", 0

    try:
        start_time = time.time()
        # text-generation ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã«ã¯æ–‡å­—åˆ—ã ã‘æ¸¡ã—ã¾ã™
        full_text = pipe(
            user_question,
            max_new_tokens=512,
            do_sample=True,
            temperature=0.7,
            top_p=0.9
        )[0]["generated_text"]

        # ãƒ¦ãƒ¼ã‚¶ãƒ¼å…¥åŠ›ä»¥é™ã‚’æŠ½å‡ºï¼ˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãŒæ®‹ã‚‹å ´åˆã®ç°¡æ˜“å‡¦ç†ï¼‰
        response = full_text.split(user_question, 1)[-1].strip()
        end_time = time.time()

        return response, end_time - start_time

    except Exception as e:
        st.error(f"å›ç­”ç”Ÿæˆä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        return f"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}", 0

# --- Streamlit ã‚¢ãƒ—ãƒªæœ¬ä½“ã§ã®å‘¼ã³å‡ºã—ä¾‹ ---
if __name__ == "__main__":
    st.title("LLM Chat ãƒ‡ãƒ¢")

    # (1) ãƒ¢ãƒ‡ãƒ«é¸æŠ UI
    model_name = select_model_ui()

    # (2) ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰
    pipe = load_model(model_name)

    # (3) è³ªå•å…¥åŠ› & å›ç­”ç”Ÿæˆ
    user_input = st.text_input("ã‚ãªãŸã®è³ªå•", "")
    if user_input:
        answer, latency = generate_response(pipe, user_input)
        st.markdown(f"**å›ç­”**: {answer}")
        st.caption(f"ç”Ÿæˆæ™‚é–“: {latency:.2f} ç§’")
